# PuroCheck Agent

A sophisticated RAG (Retrieval-Augmented Generation) agent that evaluates biochar project eligibility against Puro.earth standards. The agent uses advanced document parsing, vector search, and LLM reasoning to assess compliance with biochar methodology requirements.

## Features

- ğŸ” **Multi-format Document Processing**: Supports PDF, DOCX, TXT, CSV, and Excel files
- ğŸ¤– **Dual LLM Support**: Compatible with OpenAI GPT and Groq models
- ğŸ“Š **Vector Search**: ChromaDB-powered semantic search for relevant document sections
- âœ… **Compliance Evaluation**: Automated assessment against Puro.earth biochar methodology
- ğŸ“ˆ **Detailed Reporting**: Comprehensive evaluation results with evidence tracking
- ğŸ”„ **Flexible Configuration**: Customizable checklists and search parameters

## Installation

### Prerequisites

- Python 3.8 or higher
- pip (Python package installer)

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd purocheck-agent
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
# venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Set Up Environment Variables

Create a `.env` file in the root directory and add your API keys:

```bash
# For OpenAI (if using GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# For Groq (if using Groq models)
GROQ_API_KEY=your_groq_api_key_here

# Optional: Set default model provider (auto, openai, or groq)
DEFAULT_API_PROVIDER=auto
```

**Note**: You need at least one API key. The agent will automatically detect which provider to use based on available keys.

### Step 5: Prepare Your Documents

Place your project documents in the `data/` folder. Supported formats:

- PDF files (`.pdf`)
- Word documents (`.docx`)
- Text files (`.txt`)
- CSV files (`.csv`)
- Excel files (`.xlsx`)

## Usage

### Basic Usage

Run the agent with default settings:

```bash
python main.py
```

### Advanced Usage

You can customize the evaluation by modifying the parameters in `main.py` or by using command-line arguments:

```python
# Example: Force rebuild vector store and use specific model
results = agent.evaluate_checklist(
    checklist_path="checklist/sample_checklist.json",
    force_rebuild_vectorstore=True,
    api_provider="openai",  # or "groq" or "auto"
    model_name="gpt-4"      # or specific Groq model
)
```

### Configuration Options

- **`force_rebuild_vectorstore`**: Set to `True` to rebuild the vector database from scratch
- **`api_provider`**: Choose between "openai", "groq", or "auto" (default)
- **`model_name`**: Specify the exact model to use (e.g., "gpt-4", "mixtral-8x7b-32768")

## Project Structure

```
purocheck-agent/
â”œâ”€â”€ agent/                      # Core agent implementation
â”‚   â”œâ”€â”€ agent.py               # Main agent orchestrator
â”‚   â”œâ”€â”€ evaluator.py          # Evaluation logic
â”‚   â”œâ”€â”€ nodes.py              # Document processing nodes
â”‚   â”œâ”€â”€ graph.py              # Workflow graph definition
â”‚   â””â”€â”€ prompts.py            # LLM prompts
â”œâ”€â”€ checklist/                 # Evaluation criteria
â”‚   â”œâ”€â”€ sample_checklist.json # Default Puro.earth checklist
â”‚   â””â”€â”€ schema.py             # Checklist validation schema
â”œâ”€â”€ data/                      # Document storage (ignored by git)
â”œâ”€â”€ chroma_db/                # Vector database (ignored by git)
â”œâ”€â”€ main.py                   # Application entry point
â”œâ”€â”€ requirements.txt          # Python dependencies
â””â”€â”€ .env                      # Environment variables (create this)
```

## API Keys Setup

### OpenAI API Key

1. Go to [OpenAI Platform](https://platform.openai.com/)
2. Create an account or log in
3. Navigate to API Keys section
4. Create a new API key
5. Add it to your `.env` file

### Groq API Key

1. Go to [Groq Console](https://console.groq.com/)
2. Create an account or log in
3. Generate an API key
4. Add it to your `.env` file

## Customizing Evaluation Criteria

You can modify the evaluation checklist by editing `checklist/sample_checklist.json`. Each requirement can include:

- **`requirement`**: Description of what needs to be checked
- **`searchKeywords`**: Keywords to guide document search
- **`category`**: Grouping for organization

## Output

The agent generates detailed evaluation results including:

- **Summary**: Overall project status and statistics
- **Section-by-section analysis**: Detailed findings for each requirement
- **Evidence tracking**: Source documents and confidence scores
- **Missing evidence**: What documentation might be needed

Results are saved to `evaluation_results.json`.

## Troubleshooting

### Common Issues

1. **Import Errors**: Ensure all dependencies are installed via `pip install -r requirements.txt`
2. **API Key Errors**: Verify your API keys are correctly set in the `.env` file
3. **Vector Store Issues**: Try setting `force_rebuild_vectorstore=True` to rebuild the database
4. **Document Processing Errors**: Ensure documents are in supported formats and not corrupted

### Debug Mode

For detailed logging, you can modify the logging level in the code or check the console output for diagnostic information.
