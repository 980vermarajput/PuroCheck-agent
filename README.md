# PuroCheck Agent

A sophisticated RAG (Retrieval-Augmented Generation) agent that evaluates biochar project eligibility against Puro.earth standards. The agent uses advanced document parsing, vector search, and LLM reasoning to assess compliance with biochar methodology requirements.

## Features

- üîç **Multi-format Document Processing**: Supports PDF, DOCX, TXT, CSV, and Excel files
- ü§ñ **Dual LLM Support**: Compatible with OpenAI GPT and Groq models with automatic API detection
- üìä **Vector Search**: ChromaDB-powered semantic search for relevant document sections
- ‚úÖ **Compliance Evaluation**: Automated assessment against Puro.earth biochar methodology
- ÔøΩ **Robust Error Handling**: Exponential backoff retry logic for API rate limits and token errors
- ÔøΩüìà **Detailed Reporting**: Comprehensive evaluation results with evidence tracking and source attribution
- ÔøΩ **Flexible Configuration**: Customizable checklists, search parameters, and force rebuild options
- üéØ **Token Optimization**: Automatic context size adjustment for different API providers

## Installation

### Prerequisites

- Python 3.8 or higher
- pip (Python package installer)

### Step 1: Clone the Repository

```bash
git clone <repository-url>
cd purocheck-agent
```

### Step 2: Create Virtual Environment (Recommended)

```bash
# Create virtual environment
python -m venv venv

# Activate virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
# venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Set Up Environment Variables

Create a `.env` file in the root directory and add your API keys:

```bash
# For OpenAI (if using GPT models)
OPENAI_API_KEY=your_openai_api_key_here

# For Groq (if using Groq models)
GROQ_API_KEY=your_groq_api_key_here

# Optional: Set default model provider (auto, openai, or groq)
DEFAULT_API_PROVIDER=auto
```

**Note**: You need at least one API key. The agent will automatically detect which provider to use based on available keys.

### Step 5: Prepare Your Documents

Place your project documents in the `data/` folder. Supported formats:

- PDF files (`.pdf`)
- Word documents (`.docx`)
- Text files (`.txt`)
- CSV files (`.csv`)
- Excel files (`.xlsx`)

## Usage

### Basic Usage

Run the agent with default settings:

```bash
python main.py
```

### Advanced Usage

You can customize the evaluation by modifying the parameters in `main.py` or by using command-line arguments:

```python
# Example: Force rebuild vector store and use specific model
results = agent.evaluate_checklist(
    checklist_path="checklist/sample_checklist.json",
    force_rebuild_vectorstore=True,
    api_provider="openai",  # or "groq" or "auto"
    model_name="gpt-4"      # or specific Groq model
)
```

### Configuration Options

- **`force_rebuild_vectorstore`**: Set to `True` to rebuild the vector database from scratch
- **`api_provider`**: Choose between "openai", "groq", or "auto" (default)
- **`model_name`**: Specify the exact model to use (e.g., "gpt-4", "mixtral-8x7b-32768")

## Project Structure

```
purocheck-agent/
‚îú‚îÄ‚îÄ agent/                      # Core agent implementation
‚îÇ   ‚îú‚îÄ‚îÄ agent.py               # Main agent orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py          # Evaluation logic
‚îÇ   ‚îú‚îÄ‚îÄ nodes.py              # Document processing nodes
‚îÇ   ‚îú‚îÄ‚îÄ graph.py              # Workflow graph definition
‚îÇ   ‚îî‚îÄ‚îÄ prompts.py            # LLM prompts
‚îú‚îÄ‚îÄ checklist/                 # Evaluation criteria
‚îÇ   ‚îú‚îÄ‚îÄ sample_checklist.json # Default Puro.earth checklist
‚îÇ   ‚îî‚îÄ‚îÄ schema.py             # Checklist validation schema
‚îú‚îÄ‚îÄ data/                      # Document storage (ignored by git)
‚îú‚îÄ‚îÄ chroma_db/                # Vector database (ignored by git)
‚îú‚îÄ‚îÄ main.py                   # Application entry point
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ .env                      # Environment variables (create this)
```

## API Keys Setup

### OpenAI API Key

1. Go to [OpenAI Platform](https://platform.openai.com/)
2. Create an account or log in
3. Navigate to API Keys section
4. Create a new API key
5. Add it to your `.env` file

### Groq API Key

1. Go to [Groq Console](https://console.groq.com/)
2. Create an account or log in
3. Generate an API key
4. Add it to your `.env` file

## Customizing Evaluation Criteria

You can modify the evaluation checklist by editing `checklist/sample_checklist.json`. Each requirement can include:

- **`requirement`**: Description of what needs to be checked
- **`searchKeywords`**: Keywords to guide document search
- **`category`**: Grouping for organization

## Output

The agent generates detailed evaluation results including:

- **Summary**: Overall project status and statistics
- **Section-by-section analysis**: Detailed findings for each requirement
- **Evidence tracking**: Source documents and confidence scores
- **Missing evidence**: What documentation might be needed

Results are saved to `evaluation_results.json`.

## Troubleshooting

### Common Issues

1. **Import Errors**: Ensure all dependencies are installed via `pip install -r requirements.txt`
2. **API Key Errors**: Verify your API keys are correctly set in the `.env` file
3. **Vector Store Issues**: Try setting `force_rebuild_vectorstore=True` to rebuild the database
4. **Document Processing Errors**: Ensure documents are in supported formats and not corrupted
5. **API Rate Limits**: The agent automatically handles rate limits with exponential backoff retry logic
6. **Token Limit Errors**: Context size is automatically optimized for different API providers (Groq uses smaller context)

### Retry Logic

The agent includes robust error handling for API failures:

- **Rate Limits (429)**: Automatic retry with exponential backoff (1s, 2s, 4s, 8s, 16s)
- **Token Limits (413)**: Automatic context reduction and retry
- **API Errors (4xx/5xx)**: Up to 5 retry attempts before terminating evaluation
- **Graceful Termination**: If all retries fail, evaluation stops with clear error reporting

### Debug Mode

For detailed logging, you can modify the logging level in the code or check the console output for diagnostic information.
